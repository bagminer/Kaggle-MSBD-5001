{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6271f5c09bdebe621542a351c5f908a0e57a3399"
   },
   "outputs": [],
   "source": [
    "## Reading the file from train and test csv files\n",
    "\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "df1 = pd.read_csv(\"train.csv\")\n",
    "df2 = pd.read_csv(\"train_old.csv\")\n",
    "train = pd.concat ([df1,df2], sort=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing and feature engineering ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_jobs was assigned to the max core available in the system i.e., 12 (otherwise for regression -1 will create issues.)\n",
    "\n",
    "train.loc[train['n_jobs'] == -1, 'n_jobs'] = 12\n",
    "test.loc[train_1['n_jobs'] == -1, 'n_jobs'] = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "65e9af2441510f0355e7142519c626050d283523"
   },
   "outputs": [],
   "source": [
    "## Feature Engineering. ##\n",
    "# Creating 3 features after trying with different combinations using n_classes, n_classes_per_class, n_jobs and max_iter as all of them are interrelated.\n",
    "\n",
    "train['max_samPerJob'] = (train['max_iter'] * train['n_samples'])/train['n_jobs']\n",
    "train['n_clusPerJob'] = (train['n_classes'] * train['n_clusters_per_class'])/train['n_jobs']\n",
    "train['clus_info'] = train['n_clusPerJob']/train['n_informative']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "66810d95755b48712991ecd426a7c36f360aceff"
   },
   "outputs": [],
   "source": [
    "test['max_samPerJob'] = (test['max_iter'] * test['n_samples'])/test['n_jobs']\n",
    "test['n_clusPerJob'] = (test['n_classes'] * test['n_clusters_per_class'])/test['n_jobs']\n",
    "test['clus_info'] = test['n_clusPerJob']/test['n_informative']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "42b13d62579dade7be16cd1422af05373afa858e"
   },
   "outputs": [],
   "source": [
    "y_time = train[\"time\"]\n",
    "pen_train = train[\"penalty\"]\n",
    "pen_test = test[\"penalty\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c8ba641f85abe8bf3392c3e79d6392b4178b2348"
   },
   "outputs": [],
   "source": [
    "train = train.drop(['time'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "116609d5669f810c465f7b0872066e6c574f2d00"
   },
   "outputs": [],
   "source": [
    "## Normalizing the data using standard scalar\n",
    "\n",
    "num_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "train_num = df2.select_dtypes(include=num_dtypes)\n",
    "test_num = test.select_dtypes(include=num_dtypes)\n",
    "\n",
    "train_std = (train_num - test_num.mean())/test_num.std(ddof=0)\n",
    "\n",
    "train_final = pd.concat([pen,train_std], axis=1)\n",
    "\n",
    "train_final = train_final.drop(columns=['l1_ratio','scale','random_state','alpha','flip_y'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "426592700026d0007d891c51d6b6fc0a600df9ff"
   },
   "outputs": [],
   "source": [
    "test_std = (test_num - test_num.mean())/test_num.std(ddof=0)\n",
    "test_final = test_std.join(pen1)\n",
    "test_final = test_final.drop(columns=['l1_ratio','scale','random_state','alpha','flip_y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training of the model using DNNLinearCombinedRegressor ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "09067b4520b8ab33b4d1b790da838e91f084f83c"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 150\n",
    "num_epochs = 10000\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_final, y_time, test_size=0.3) \n",
    "\n",
    "input_train = tf.estimator.inputs.pandas_input_fn(x=train_final,y=y_time,batch_size=BATCH_SIZE,num_epochs=num_epochs,shuffle=True)\n",
    "\n",
    "input_test = tf.estimator.inputs.pandas_input_fn(x=X_test,y=y_test,batch_size=BATCH_SIZE,num_epochs=num_epochs,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "10f5ca8cab5718855f5731a211610b9a490fa9ec"
   },
   "outputs": [],
   "source": [
    "n_classes = tf.feature_column.numeric_column(\"n_classes\")\n",
    "n_clusters_per_class = tf.feature_column.numeric_column(\"n_clusters_per_class\")\n",
    "n_informative = tf.feature_column.numeric_column(\"n_informative\")\n",
    "n1 = tf.feature_column.numeric_column(\"n_clusPerJob\")\n",
    "n2 = tf.feature_column.numeric_column(\"max_samPerJob\")\n",
    "n3 = tf.feature_column.numeric_column(\"clus_info\")\n",
    "max_iter = tf.feature_column.numeric_column(\"max_iter\")\n",
    "n_jobs = tf.feature_column.numeric_column(\"n_jobs\")\n",
    "n_samples = tf.feature_column.numeric_column(\"n_samples\")\n",
    "n_features = tf.feature_column.numeric_column(\"n_features\")\n",
    "\n",
    "penalty = tf.feature_column.categorical_column_with_vocabulary_list(key=\"penalty\", vocabulary_list=[\"l2\", \"l1\", \"none\", \"elasticnet\"])\n",
    "pen = tf.feature_column.indicator_column(penalty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "181455a3a1d48a527b17f5434c1cc1f87ab6526d"
   },
   "outputs": [],
   "source": [
    "\n",
    "linear_feat_col = [    \n",
    "    max_iter,\n",
    "    n_jobs, \n",
    "    n_samples, \n",
    "    n_features, \n",
    "    n_classes, \n",
    "    n_clusters_per_class, \n",
    "    n_informative,\n",
    "    n1,\n",
    "    n2,\n",
    "    n3,\n",
    "]\n",
    "\n",
    "DNN_feat_col = [\n",
    "    max_iter,\n",
    "    n_jobs, \n",
    "    n_samples, \n",
    "    n_features, \n",
    "    n_classes,\n",
    "    n_clusters_per_class, \n",
    "    n_informative,\n",
    "    n1,\n",
    "    n2,\n",
    "    n3,\n",
    "    pen,\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9bda391f11f8399e72433304b0f9078be29fad25"
   },
   "outputs": [],
   "source": [
    "model = tf.estimator.DNNLinearCombinedRegressor(\n",
    "    linear_feature_columns=linear_feat_col,\n",
    "    dnn_feature_columns=DNN_feat_col,\n",
    "    dnn_hidden_units=[1500, 550, 360, 150, 75, 25, 14,7, 3],\n",
    "    dnn_activation_fn=tf.nn.leaky_relu)\n",
    "model.train(input_fn=input_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction on test data ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b9eeedb46938007efa5723f5d5f97c21310dcc9f"
   },
   "outputs": [],
   "source": [
    "pred_inp_fn = tf.estimator.inputs.pandas_input_fn(\n",
    "        x=test,\n",
    "        batch_size=1,\n",
    "        num_epochs=1,\n",
    "        shuffle=False)\n",
    "\n",
    "preds = m.predict(input_fn=pred_inp_fn)\n",
    "res = []\n",
    "for i in preds:\n",
    "    res.append(i[\"predicted_label\"][0])\n",
    "print(res)\n",
    "\n",
    "print(len(res))\n",
    "\n",
    "c=0\n",
    "for i in res:\n",
    "    if i<0:\n",
    "        c += 1\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1ca265e3e44ec75e1b90c9a42f2fe07145fa6c95"
   },
   "outputs": [],
   "source": [
    "test_id = np.arange(100)\n",
    "test_id = test_id.reshape(len(test_id),1)\n",
    "res = np.array(res)\n",
    "res = res.reshape(len(res),1)\n",
    "res = np.abs(res)\n",
    "out = np.concatenate((test_id,res), axis=1)\n",
    "np.savetxt(\"Submission.csv\", out, delimiter=\",\", fmt='%i,%f', header=\"Id,Time\", comments='')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
